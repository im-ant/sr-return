# ========================================================================
# Based mainly on the Rainbow hyperparameters:
# https://arxiv.org/abs/1710.02298.pdf
# ========================================================================
defaults:
  - optim: adam_jax_dqn
  - agent: dqn_nature_rainbow
experiment:
  log_interval: 1e3
  save_interval: 1e5  # save per 100k steps
  eval_interval: 1e5  # eval per 100k steps
training:
  seed: 1
  max_steps: 2e7
  discount: 0.99
  history_length: 4
  target_network_update_freq: 8000  # 8k updates in Rainbow (32k frames), 10k updates in original
  exploration_steps: 80000  # 80k frames in Rainbow (50k frames in DQN)
  double_q: False
  async_actor: False
  random_action_schedule:
    start: 1.0
    end: 0.01
    steps: 250000  # 250k frames in Rainbow, 1M frames in original
env:
  game: 'BreakoutNoFrameskip-v4'  # BreakoutNoFrameskip-v4, MsPacmanNoFrameskip-v4
agent:
  cls_string: 'DQNAgent'
  kwargs: {}
network:
  cls_string: 'VanillaNet'
  kwargs: {}
replay_buffer:
  kwargs:
    memory_size: 1e6
    batch_size: 32
    n_step: 1
  asynch: False
optim:
  sgd_update_frequency: 4
  gradient_clip: 5
  kwargs: {}
